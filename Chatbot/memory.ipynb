{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735ac1ef",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "In this video We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6cfb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0881a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x114d9f2b0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x114d9ce80>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942d377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Vatsal, it's nice to meet you! I am Gemma, an open-weights AI assistant. I don't have a last name. üòä\\n\\nWhat can I do for you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 29, 'total_tokens': 75, 'completion_time': 0.083636364, 'prompt_time': 0.001401549, 'queue_time': 0.188514311, 'total_time': 0.085037913}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c38180e6-5161-4e5d-af98-97d51cc461cb-0', usage_metadata={'input_tokens': 29, 'output_tokens': 46, 'total_tokens': 75})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Vatsal. I am a AI Engineer. What is your name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ac5898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a large language model, I don't have personal preferences like a favorite programming language. I can process and understand many different languages, including Python, Java, C++, and more.\\n\\nWhich language do you enjoy working with the most? I'd love to learn more about it!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 68, 'total_tokens': 130, 'completion_time': 0.112727273, 'prompt_time': 0.002277108, 'queue_time': 0.188888188, 'total_time': 0.115004381}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--71090893-4520-4956-80e2-674753d880fe-0', usage_metadata={'input_tokens': 68, 'output_tokens': 62, 'total_tokens': 130})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Hi, My name is Vatsal. I am a AI Engineer. What is your name?\"),\n",
    "AIMessage(content=\"Hello Vatsal, I am an AI model created by Groq. How can I assist you today?\"),\n",
    "HumanMessage(content=\"What is your favorite programming language?\")]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dc5a0",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7574788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29e23f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_history = RunnableWithMessageHistory(model, get_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2bd9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b101d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, My name is Vatsal. I am a AI Engineer. What is your name?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "178d4b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Vatsal, it's nice to meet you!\\n\\nMy name is Gemma. I'm an AI assistant, here to help with any questions or tasks you might have.  \\n\\nWhat kind of AI work are you involved in?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 103, 'total_tokens': 156, 'completion_time': 0.096363636, 'prompt_time': 0.002852148, 'queue_time': 0.18865328, 'total_time': 0.099215784}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--19e2a00e-7ece-4a53-80c4-e347b2bab12a-0', usage_metadata={'input_tokens': 103, 'output_tokens': 53, 'total_tokens': 156})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c41e2e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Vatsal.  \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 169, 'total_tokens': 191, 'completion_time': 0.04, 'prompt_time': 0.003931258, 'queue_time': 0.18789434700000002, 'total_time': 0.043931258}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a021654a-acfa-406b-ae9d-ae2dd47a828f-0', usage_metadata={'input_tokens': 169, 'output_tokens': 22, 'total_tokens': 191})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_history.invoke([HumanMessage(content=\"What is my name?\")], \n",
    "config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae3e19f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'd be happy to use it!\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2922c941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, John! üëã \\n\\nHow can I help you today? üòä\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d4f7eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John, remember?  üòä  \\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26353f4",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7816669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the question to the nest of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c1e5bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Vatsal! It's nice to meet you. \\n\\nI'm happy to answer any questions you have to the best of my ability.  What can I help you with today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 32, 'total_tokens': 79, 'completion_time': 0.085454545, 'prompt_time': 0.001485859, 'queue_time': 0.18880281899999998, 'total_time': 0.086940404}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ce1ac286-2748-4a5a-a287-27c17d879230-0', usage_metadata={'input_tokens': 32, 'output_tokens': 47, 'total_tokens': 79})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Vatsal\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b2c3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_history = RunnableWithMessageHistory(model, get_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7324a262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Vatsal, it's nice to meet you! üëã\\n\\nWhat can I do for you today? üòä  \\n\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "\n",
    "response=with_history.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Vatsal\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "241ac6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Vatsal.  \\n\\nI remember! üòâ  Would you like to talk about something else?\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_history.invoke({\"messages\":[HumanMessage(content=\"What is my name?\")]},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62472c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b54f298c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§µ‡§§‡•ç‡§∏‡§≤! üëã \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§ \\n\\n‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§™‡•Ç‡§õ‡§®‡§æ ‡§ö‡§æ‡§π‡•á‡§Ç‡§ó‡•á?  üòä\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Vatsal\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8e90681",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfbecd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§µ‡§§‡•ç‡§∏‡§≤! üëã  ‡§Æ‡•Å‡§ù‡•á ‡§ñ‡§º‡•Å‡§∂‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§ \\n\\n‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡•á‡§Ç‡§ó‡•á? üòä \\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "\n",
    "repsonse=with_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Vatsal\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50af5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97c86df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§µ‡§§‡•ç‡§∏‡§≤ ‡§π‡•à! üòä  \\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68796e",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f85d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6699403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vatsal/Machine Learning/Gen AI/LangChain/lang/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72392abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "def get_last_n_messages(messages, n):\n",
    "    return messages[-n:]\n",
    "\n",
    "result= get_last_n_messages(messages, 5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84ce9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "835fe0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your last three messages were:\\n\\n1. thanks\\n2. no problem!\\n3. having fun?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain= (RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    " | prompt \n",
    " | model)\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What was my last 3 message?\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e47bd92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the MEssage History\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}\n",
    "with_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "145b4379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The math problem was 2 + 2. \\n\\nIs there anything else I can help you with? üòä\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats was the math problem?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bc35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
